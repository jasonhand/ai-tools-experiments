---
title: "Claude Code findings with Rust and TypeScript through Ray Tracing projects"
description: "In this candid engineering chat, Datadog developer advocate Scott Gerring walks Jason Hand through his latest experiments with Anthropic's Claude Code assistant. Gerring shows off a ray-traced teapot scene that Claude generated almost entirely on its own."
heroImg: "../images/thumbnails/ep32.png"
videoId: "MD7L1HdCpX0"
episodeNumber: 32
date: "2025-06-11T18:42:35.000Z"
author: "jasonhand24@gmail.com"
participants: ["Jason Hand", "Scott Gerring"]
tags: ["claude-code", "typescript", "rust"]

takeaways:
  - text: "Generative coding assistants like Claude Code can produce surprisingly correct, compiler-validated architecture when given clear conversational guidance"
    tools: ["Claude Code", "Rust"]
  - text: "AI tools also invent highly 'creative' bugs—unsafe globals, mutex bottlenecks, missing epsilon offsets—that require domain expertise to detect"
    tools: ["Claude Code", "Rust"]
  - text: "For rote tasks (type definitions, data mapping, API clients, refactors) Claude saves hours, especially in TypeScript and Astro projects"
    tools: ["Claude Code", "TypeScript", "Astro"]
  - text: "Complex build or tooling changes (e.g., JavaScript module systems, Rust compiler internals) remain fragile; human review and testing are essential"
    tools: ["JavaScript", "Rust", "Testing"]
  - text: "Multi-modal workflows—hand-written drafts, observability data, shared context stores—hint at a future where AI augments the entire software lifecycle, not just code editing"
    tools: ["AI Development", "Workflow Tools"]

resources:
  - name: "Claude Code"
    url: "https://claude.ai"
    description: "Anthropic's AI coding assistant"
  - name: "Datadog DevHub"
    url: "https://devhub.datadoghq.com"
    description: "Integration and extensibility hub for Datadog platform"
  - name: "Astro static-site framework"
    url: "https://astro.build"
    description: "Modern static site builder for fast websites"
  - name: "Supernote e-ink writing tablets"
    url: "https://www.supernote.com"
    description: "Digital writing tablets for handwritten notes"

jumpTo:
  - title: "Intro & recap of previous experiments"
    url: "https://youtu.be/MD7L1HdCpX0?t=0"
    timestamp: "00:00:00"
  - title: "Ray-traced teapots generated by Claude Code"
    url: "https://youtu.be/MD7L1HdCpX0?t=60"
    timestamp: "00:01:00"
  - title: "Creative bugs – static globals & unsafe Rust"
    url: "https://youtu.be/MD7L1HdCpX0?t=420"
    timestamp: "00:07:00"
  - title: "DevHub ETL, Astro, and Claude's TypeScript wins"
    url: "https://youtu.be/MD7L1HdCpX0?t=960"
    timestamp: "00:16:00"
  - title: "Lessons learned, handwriting workflows & wrap-up"
    url: "https://youtu.be/MD7L1HdCpX0?t=1680"
    timestamp: "00:28:00"

summary: "In this candid engineering chat, Datadog developer advocate Scott Gerring walks Jason Hand through his latest experiments with Anthropic's Claude Code assistant. Gerring shows off a ray-traced teapot scene that Claude generated almost entirely on its own. By conversationally prescribing high-level architecture and letting the model write the Rust, he ended up with clean, compiler-checked interfaces for materials, impressive Lambertian shading and glass refractions, and a working multithreaded renderer—all things that would have sounded like science-fiction two years ago. Yet the demo also exposes Claude's more comedic failure modes: a 'random' axis picker implemented as a global static counter wrapped in an unsafe block, a crippling global mutex that turns a progress bar into a five-times performance penalty, and subtle rendering artifacts caused by omitting the classic ray-origin epsilon offset. Beyond graphics, Gerring describes using Claude Code as a pragmatic coding mule. It rapidly scaffolded a TypeScript ETL pipeline that ingests Datadog integration metadata into the new DevHub site powered by Astro, complete with strongly-typed schemas and a bespoke GitHub API client. The assistant excelled at mechanical mapping and refactoring chores that normally burn hours of human focus, but it stumbled when asked to wire up Jest unit-tests—introducing brittle module-system changes that broke the entire build. At the bleeding edge, Claude still falters on Rust compiler internals for architectural linting, reminding us that engineers must recognise success criteria and domain pitfalls before blindly shipping AI-generated code. The pair close by musing about shared organisational context windows, multi-modal workflows like handwriting drafts on a Supernote then piping PDFs through Claude for flawless transcription, and the exciting potential of Datadog's new MCP for autonomous incident remediation. The through-line: lean on generative AI for the repetitive drudgery, but keep seasoned intuition—plus good observability—at the helm."

transcriptContent: "[00:00:00] **Jason Hand:** I heard you've you've been doing a little bit more with some of the stuff we talked about.

[00:00:04] **Scott Gerring:** I think, the main thing we talked about last was I.

Kind of my fancy terminal stuff. It's not that fancy really, but I think lots of people who haven't spent much time in the terminal before are probably quite interested in that. And then Claude Code was the thing that I was using that was novel in there. But as part of that whole thing, the whole thing, I've been building with Claude and using the terminal and thinking about programming and generative AI and whatnot.

A lot recently, probably like everyone who's programming, I would say I have some observations and some other things and bits of code that went well and bits of code that didn't go well and so on that are probably interesting to look at. I think, 

[00:00:39] **Jason Hand:** maybe you can pull up your screen here and show us the next level of understanding that you've come to with a lot of these things.

[00:00:47] **Scott Gerring:** Yeah . I'll show you my teapots first really quickly because as I say, still very into the teapots. I'll just quickly switch desktops here for a second. This is the output that we're getting out of this thing that I've built with a ray [00:01:00] tracer. With code, it looks about right. It's very noisy because that's what, ray trace renders look like when you don't run them for ages, and I don't have days and days to wait for my shiny teapots, but it's it's roughly okay.

And I the whole thing I did with it was, Hey, Claude Code, go and build this thing. And I gave it advice about how to build it based on my knowledge of how to build these things. So I was very prescriptive with it, but only conversationally, and I left the code completely alone. Until it got to the point where I had my shiny teapots and then I went and had a bit of a poke around to see what fun things I could find.

And there's a mix of stuff that went really well. I think, obviously the teapots came out the end, which is effectively magic. I think if you, if. I told someone two years ago, a computer program could write other computer programs for you and they'd be pretty decent. They'd be very surprised about that and stuff.

That's like pretty horrible and gross, but like discoverable and fixable, I would say. 

[00:01:50] **Jason Hand:** Yeah. I, another thing too, you, it took what we talked about in the recording and then you did a brown bag for our team internally and everybody loved it. And [00:02:00] you had made a comment or there was a section in your presentation about just the grossness of the code and 

[00:02:05] **Scott Gerring:** Yeah, I get strange, I said like strange guttural reactions to things sometimes, but it's, I think it's the creativity of some of the failure modes I guess.

But it was also really interesting, the bag, I thought, because obviously our colleague Rory. Also did half the brown bag and he was also talking about generative ai. And we basically we built two completely different things. Like I built this ray racer and he built a system for auditing Kubernetes control plan actions, as I recall.

[00:02:32] **Jason Hand:** And. 

[00:02:33] **Scott Gerring:** The domain was very different, but the conclusions at the end were very similar about when you can use it effectively when you can't. And I thought that was really interesting. And it comes back to what you were saying about the community call. Lots of people are playing with this stuff and everyone ends up with the same Yeah, very similar opinions about it.

[00:02:47] **Jason Hand:** Exactly. It feels to me like we've all, we're all taking our time and taking turns, like entering this new mysterious world that we don't really understand, everything around and we're all just reaching for the edges, reaching [00:03:00] for the walls. What, how far can I go before like I'm in danger?

Or how far can I go before this is not useful? You know what I mean? We're just all like very carefully exploring and then turning that around and being like, Hey, here's what I found out. 

[00:03:14] **Scott Gerring:** Pass it on. Yeah, exactly. And I think that's also the way to do it with new tools. Like you want to crash at high speed into the boundaries rather than like incrementally working out from what's safe and reasonable.

But yeah. Anyway, let me show you some code. Yeah. And I'll start with the positive stuff because I was very skeptical about the whole generative AI coding assistant thing to start with. I think lots of programmers have been doing it for a while. Work. But it does a lot of stuff really well, so I think it's important to focus on that rather than just getting the boot in immediately.

[00:03:43] **Jason Hand:** Agreed. 

[00:03:44] **Scott Gerring:** This is a bit of the Ray tracer and I don't need you or everyone listening to understand what's going on too much, but the important part is that it's an interface. So it's an interface that represents [00:04:00] some material in your Ray tracer. That can reflect light, which means anything that you render, like you have a shiny piece of metal or you have some diffuse surface, it can reflect light.

And this interface describes how that material reflects light. And it's really good because it's correct, right? Like it looks exactly like what you would expect this interface to look like if you go on Google, the myriad of rate racing tutorials on the internet. And the cool upshot of that is that because it's an interface and all the different materials that Claude Code went and built for me in my little toy ray tracer.

Inherit or implement this interface, they're all forced to look roughly correct. And I think because it's this compiler check thing where it gets the interface right, and then it goes and builds a bunch of code that has to force that shape. It really helps it get the materials right as well. So it's like relying on the compiler to do the compiler's job, to push it in the right direction.

So it starts from something that's pretty correct and then it extends out from there. So this doesn't really tell you much more, but this is a concrete example of that this is a lambertian material, which just means like a [00:05:00] diffuse surface saying it's not very reflective and it's shaped again, pretty much like what you would expect.

But I think because the foundational interface was right and the code follows from that. So I looked at this and I was like, oh, hey, this is exactly what I've written, would've written myself. Great job. 

[00:05:16] **Jason Hand:** Now when you say it's right, is that what you mean? It's what you would have done? Or is there objectively a right and wrong way of doing this that you can write tests around?

[00:05:27] **Scott Gerring:** It's probably a little bit more nuanced than I made it sound. There's lots of different ways you can build Ray races, and I'm by no means an expert, right? Like I'm a hobbyist with an enthusiasm for these things. But if you go and do like a mid-tier path tracing sort of thing, you would end up with something that is very similar to this.

So there's not really an objective truth, but you would expect an evaluator sample and a PDF function on it, and you would expect roughly these parameters. There are variants, but it's insofar as you could be objectively right with this is. [00:06:00] So I think this is really cool. Like it's spot on.

You look at it, you're like, nice one. If anyone who is listening knows more about Ray Races than me and would like to tell me about subtle nuance in this please let me know because I'm always interested in letting you might be the expert, very much doubt that. But yeah, I mean that there are cool toy problem.

Cool. And there was another one of this. It's the same sort of thing. So this is for a material, it tells you how light scatters when it hits it. It's pretty similar again. And it has the same nice property where things that extend this are forced into the correct shape by virtue of it. Having got a decent interface out

now for the funnel ones or the ones that I think are funnel some stuff that didn't go so well. And we'll start off with this, which I have called Static Globals. And I need to explain this 'cause I think it's very creative. It's a function that's meant to pick a random axis. So X, Y, or Z, which means a random number between zero and two.

And [00:07:00] generating random numbers in rust is not a hard thing to do. This isn't something that you need to have elaborate work around for, but what it's done, instead of generating a random number and you can see in its comment that it thinks maybe it should actually generate a random number, is create a global mutable static counter.

And then bump it whenever somebody calls the function from any thread and return the next value. So by virtue of lots of threads calling this thing at once, it's gonna look random, but it's just an incrementing, rotating counter. And this is really funny because, if you got a graduate programmer or someone, somebody who would just started programming and said, go and build me this, there's no way that would come up with something like this.

It's creative in its horror. So I saw it and I was like, wow, this is really something new.

[00:07:46] **Jason Hand:** I liken the comment that it says we'd normally use a proper random number generator, but Yeah. And then it goes on to write seven lines of code. 

[00:07:55] **Scott Gerring:** Yeah. And I really wish I had the chat history from it because I think it would've been one of those ones where it [00:08:00] reflects, like it, it would've tried to use it and it got the wrong package or something, and it was like, that doesn't seem to work time to do it this way instead.

But unfortunately, let's three 

[00:08:09] **Jason Hand:** force our random. 

[00:08:11] **Scott Gerring:** Yeah, and the fact that it's had to wrap this in an unsafe block is also deeply funny. This is not normally something you would see in ordinary rust code, 

[00:08:19] **Jason Hand:** Okay. Let's actually talk about that for a second. If you caught that you look at that and like almost throw up in your mouth, right?

I don't know that I would honestly, I don't know rust well enough to spot, but I wouldn't be working in Rust. I know the dangers of being in a language that I don't understand what the gen AI's created. But let's pretend. Somebody has created something and it slipped through.

What what does that set off in your mind, like alarms? 

[00:08:41] **Scott Gerring:** So in this case it's harmless. Because it ends up, I, randomness is one of those things where people take it very seriously, that something's truly random and whatnot. This is absolutely not truly random, but for the purposes of just random of picking an axis.

And the axis should change a lot and look roughly random this would [00:09:00] work. But if you ran, there'll be some rust static analysis tools. You could run over rust programs to look for things that are unpleasant and most of them would fire off on the use of unsafe blocks like that. There is a massive code smell, but functionally, I don't think it's that terrible.

It's gonna kind of work, it's just creative and it's. Horribleness, 

[00:09:21] **Jason Hand:** maybe not the most efficient use of code. A senior person's gonna come in looking for ways to reduce lines of code.

[00:09:27] **Scott Gerring:** Yeah. And they're also gonna be like, what were you thinking with this? I wasn't vibe that was bad for 

[00:09:35] **Jason Hand:** me.

[00:09:35] **Scott Gerring:** It's just that it's so creative, to do it this way, it's not something that a human would really consider, I think. And I wonder if. It would be really interesting to see what caused it to not use the random number generated. 'cause there would absolutely be something like it. It tried to use it and it didn't work for Yeah.

Some reason it's not obvious to me. And then it did this instead. 

[00:09:54] **Jason Hand:** That's an interesting point. It has caused us, you mostly now me, to pause [00:10:00] and think about the creativity of its response, and oh. You can, we can solve this problem in multiple ways. I guess we've always known that, but this is a different way.

I've never seen, and I think at some level that has to unlock something in a person where they're like, whoa, yeah, I didn't know you could do that. Or I never thought to do it that way. You know what I mean? Like I'd live off those little mortals of, oh my God, I didn't know you do that, that it's it just keeps me moving forward.

Even though you would never want to code like this, I think it's still a little bit of a learning revelation thing of we're just scratching the surface on stuff. 

[00:10:39] **Scott Gerring:** I absolutely, and I think it's very interesting, like it, it really makes me wonder how it's come to it, like what it's saying in the trading set or what error it encountered when it tried to do it with a random number generator that led to this.

What you would expect to come outta the brain of an ordinary human programmer, which is why it really, it's really [00:11:00] jarring. But yeah I have another one that's, but would you call it 

[00:11:04] **Jason Hand:** slop? Because I think that's where that term comes 

[00:11:06] **Scott Gerring:** in. It's absolutely slop. Yeah. Okay. Because 

[00:11:09] **Jason Hand:** we look at images and video and it's just it gives you that WTF feeling where you're like, Ugh, what is this?

Yeah. That's the point of those things. At least the ones that end up on social media. This is not by design to be slop. It's just slop. And, but also it's creative, and I guess if you look at it through a squint through like an artist's view, it's creative. What are you gonna say?

[00:11:33] **Scott Gerring:** Yeah. There's almost an aesthetic judgment in there, like I, you asked me if it was gonna work and I was like, yeah, it probably would, but it's disgusting. And that's me saying, I don't think this is good code. Based on prior exposure, but if it's gonna work, even if it's not truly random. 

[00:11:51] **Jason Hand:** Yeah.

[00:11:52] **Scott Gerring:** Anyway, this one is interesting. In a similar way. It's also gross, but I think I can [00:12:00] explain how it's ended up at this and it's not enough to just see the snippet, to know what's going on, but basically in the Ray tracer, it built me this little progress bar on the bottom, so you, it is like a command line tool.

You run it. A little, a hash progress bar ticks across until it's done. And because they take hours to render, typically this is helpful to know that it's not just frozen. And I saw that and I was like, oh, that's cool. That's I like terminal stuff obviously, and this is really nice. It's great.

And then while it was rendering the first time I cracked open my system monitoring stuff and I saw that it was spending like 95% of its time in the kernel. You can see like the split between user CPU time and Kernel CPU time. 

And the thing with normal ray traces is that they don't need to do anything with the outside world.

They just sit there in user space and do maths a lot until they're finished. They don't need to open file sockets, they don't need to use mu taxes, they don't need to load websites. So if you see that it's spending all this time going in and outta the kernel, then it's doing something that it shouldn't.

Yeah. So I saw that and the thing worked right? Like it [00:13:00] absolutely worked. It was just slow. But because Ray traces are also slow, it didn't really trigger me until I saw this CPU time split. So I saw that and I thought something terrible is happening in here and I wanna know what it is.

And I dug around a bit and I found this. And the gist of it is that to make this little progress bar thing work, it needs to fan in from all these different worker threads that are doing rendering to one point to update a number that reflects the progress, right? If you have a million pixels in the image, you should bump it by one every pixel, and you can work out what percentage you are done.

And it's done that by creating a single global mutex. So like a lock that one part of the program can hold at a time. And then every time a thread finishes rendering a single pixel, it has to lock that lock. And only one thing can hold the lock at once to increment a number by one. And the thing with that is, is that all of the threads, like you have a, I don't know, over 16 core CPUs, you probably have 32 actual threads.

They all spend most of the time blocked waiting [00:14:00] to hold this lock to bump a number by one. This is completely unnecessary, and it meant that the whole thing ended up spending like five times more Wow. Than it should have. 

[00:14:09] **Jason Hand:** It's 

[00:14:09] **Scott Gerring:** an expensive 

[00:14:10] **Jason Hand:** progress bar. 

[00:14:11] **Scott Gerring:** Yeah. It's crazy. And it's a funny failure mode because like I say, you don't notice it unless.

You, you dig into it a bit or you notice that the CPU usage is doing something funny or you have some mental model of roughly how slow it should be, which I didn't like I didn't realize that it was so much slower than what I remembered that I should look at, that it was only the CPU usage hint.

But the interesting thing here is I think you can see how it's got there. Rust has all these language elements that make it very hard to do things that are unsafe in kind of. Multi-processing situations. So if you have a bunch of threads and you have a piece of shared memory where you need to bump a number, for instance, you can't just share around immutable reference to this bit of memory.

Like you can't do that in rust. It just won't let you. You can try and the compiler will just say, no, that is not allowed. Have you [00:15:00] considered using a mutex? So what I think has happened here is that it's tried to do it the naive way, which would work in other languages like Java or C or whatnot. The compiler has given it a very direct suggestion to use.

To use Aex and wrap it up in an arc, and then it's done it, and the thing's compiled and it's worked, so it's pretty happy. 

[00:15:20] **Jason Hand:** Do you recall, so this is all in . Claude Code. Just as a reminder that you're, that this code was generated. Do you recall which model it was using at the time?

Was it sonnet? 

[00:15:29] **Scott Gerring:** It would've been, they just released a new one, I think, right? A week ago. I wanna 

[00:15:33] **Jason Hand:** say we, when we were chatting last, like when we recorded, I, it was probably 3.7 at the time. I think four came out just a little bit after that, but.

[00:15:46] **Scott Gerring:** How do I do it? Model? 

[00:15:47] **Jason Hand:** Yeah. 

[00:15:48] **Scott Gerring:** So what do we have? SONET four. Yeah, it was three point something because I know came out. Yeah. 

[00:15:54] **Jason Hand:** I wanna say unless you've written, unless this code was generated within the last, like two weeks or so, or week and a [00:16:00] half, it's probably 3.7. Yeah. Okay. I've noticed differences with four.

Like I've created some really interesting, I dunno, not super interesting, but surprisingly well built little demo apps in one or two shots. 

[00:16:17] **Scott Gerring:** Nice. 

[00:16:18] **Jason Hand:** With Claude Sonnet 4, and I can't put my finger on what's different about it, but I'm having a real high success rate. Cool. So I'd be curious to see what it did, what that model would do, 

[00:16:27] **Scott Gerring:** yeah, and this is a thing that I'm a bit sad about. We've talked about this before. I wish that. The tools themselves would write out a conversation log, because then you could just go back to it from a certain prompt, from just before when it did this, and you could say, you could just let it continue and see what happened.

But as it stands the production of the program, from the conversation I had with it is lost to the winds of time. Like it would be so hard to get it back into the situation to see if it was gonna do this little step here differently. 

[00:16:55] **Jason Hand:** Bringing that context through over time is a conversation we've had a few [00:17:00] times on here.

Ryan and I have chatted about it. Actually, Whitney on our team, I interviewed her last week, I wanna say two weeks ago maybe. She has been doing some work with MCP servers in Cursor and she's doing something that actually addresses exactly what you're talking about is after a session of coding.

During the session of coding. And then of course anything has to be summarized afterwards. But during the like time you're coding and talking to Claude or whatever, all of that is getting captured into other. And into other files and being logged in a little bit more human readable way, not just like taking the logs from, whatever, output and console and throwing them in places, but like actually documenting what's been done, including prompts, all that kind of stuff.

Yeah, other people were thinking the same way as I worked, had so much success on. Day one, I need to bring everything that we did with me. Like whole thing needs to come with me, not just like the code. 

[00:17:55] **Scott Gerring:** Yeah. 

There was there was a really interesting thing that I saw a few days ago.

I'm [00:18:00] sure you saw it as well. CloudFlare published a project that they've built. I can't remember what it was exactly. It'll be something to do with network re or whatnot, something very cloud. The code was entirely built by Claude, as I recall. And they didn't just publish the code, they published all of the prompts that they used to get to the state of the code.

So it's a really interesting artifact 'cause it's like a big serious engineering company saying, Hey, we built this serious thing but we did it with generative ai and here's how I've been meaning to dig into it. 'cause I think it'd be very interesting. 

[00:18:30] **Jason Hand:** Yeah, I have not seen that. I'll have to look at that though.

[00:18:32] **Scott Gerring:** Sounds I'll get through to you afterwards. 

[00:18:34] **Jason Hand:** Yeah, please do. 

[00:18:36] **Scott Gerring:** But yeah, so that one was interesting and this one here is terrifying in a. You, you were saying before, like if you saw that Rust that I showed you with the unsafe block, you wouldn't really think oh, this is terrible or whatnot, because you're not, you don't see yourself as a Rust programmer.

You don't have the sense for what might look unusual than Rust. Like I wouldn't have the sense for it in JavaScript, for instance, [00:19:00] this is more subtle than that. It's the same thing, but it's applied to the domain itself. So like the domain is a ray tracer, Ray tracing code has to look a certain way, do certain things.

This is missing something that's very important and you only know that it's missing if you've built a Ray tracer before and know what to expect here. And the gist of it is that like when you shoot array at a thing and it hits the thing, like this pencil and this glass that I handily have had handy it, it hits it and it either goes through it or it bounces off at absorbed.

And if the ray trebles onwards, like if it goes through the glass and continues out the other side, you make a new ray based on how it bends through the glass. And what you need to do is make it so that the new ray starts. It doesn't have the start point at the intersection. It has to have, it has to be moved a tiny bit away from the surface along the direction of the outgoing.

And the reason for that is that if you don't, because floating point maths is funny, some of the time it'll re intersect at the same point and you get weird artifacting in your output images. 

[00:19:59] **Jason Hand:** I think 

[00:19:59] **Scott Gerring:** there are probably [00:20:00] other ways of solving this, but like typically the thing to do is to just shift that new starting point of the new ray, a tiny bit along that outgoing redirection.

Okay. And this just doesn't have that. So I saw the images, it was rendering, and I noticed that in particular for surfaces that were glass, it had this very particular artifacting in it. And I thought, okay, something unusual is happening here. And I found that it was missing that, but only because I know that it should be there.

If you weren't familiar with Ray tracing, you would probably just say, oh, it's just how it is, and it would be very difficult to work this out. So I think this is a heinous one because it. 

[00:20:37] **Jason Hand:** It sounds like this is a landmine that you've seen before, right? Like it's that knowledge that, that just comes up with experience, the wisdom of playing with Ray Traces.

Ray Trace. Yeah. Wisdom is a 

[00:20:47] **Scott Gerring:** strong word to use in this kind. It is.

[00:20:49] **Jason Hand:** It's stuff that's not in the, it's not mentioned in the text, because you don't think about it. It's kinda an edge case almost. I think you 

[00:20:54] **Scott Gerring:** would, I think you would definitely see it mentioned.

It's a common pitfall. Yeah. But it's [00:21:00] just missed it. But I guess the point that I'm trying to make with it is that for something complex like this, or something reasonably complex, you have to understand what success looks like. Yeah. Before you let the agent rip on it, because at the end, if you can't assess whether or not it's correct or not, you have something that might be failing in a complicated way that you don't understand.

[00:21:20] **Jason Hand:** Yep. 

[00:21:20] **Scott Gerring:** And I've really, I've picked a pathological case with this thing, right? Like you, you get an image out and the image can be broken in ways that are very subtle. It's not like a web service or something. Sure. So I appreciate that, but I still think that there's an important lesson in there about absolutely how far you can go away from what you fully understand.

[00:21:37] **Jason Hand:** So what is it like just a couple lines of code that's missing or a function or one little variable with some math attached to it? How 

[00:21:45] **Scott Gerring:** much. A line, basically you would go new Ray I don't have all the context, but it'd be like new ray equals ray, new old\_start\_point plus [00:22:00] times tiny spelling.

[00:22:01] **Jason Hand:** Okay. So yeah, it's just got some basic hard coded math in there that you probably used to seeing. 

[00:22:06] **Scott Gerring:** Yeah. Yeah. You just you always have the direction and the direction will probably be like one, like a unit vector. Yeah. And you just make it really small 'cause you just wanna move it a tiny bit away so it doesn't re intersect itself.

[00:22:15] **Jason Hand:** That makes sense. 

[00:22:16] **Scott Gerring:** Yeah. Okay. Cool. So some other stuff. And now for something different DevHub, have you had a look at DevHub yet? I haven't. 

[00:22:27] **Jason Hand:** I've been in there, but I haven't really spent like good, genuine time in there. 

[00:22:30] **Scott Gerring:** So I'll show you it really quickly and I will tell you what I've used Claude code for.

So this is something that we've been building in the advocacy team at Datadog recently to capture all of the integration points and extensibility information for Datadog, like highlight projects from the community that are building around the platform essentially. It's pretty cool. Go have a go, have a look at it.

If you've before everybody go check it out. It's pretty new. It's brand spaking new. Yeah. Yeah. We're very excited about where we can take it. But so the co the code part of this, the part of it that I spent [00:23:00] a bit of time on was a ETL pipeline, like extract, transform, load for all the different data sources we have about existing integrations for Datadog, so things like the agent extensions, third party libraries.

We have this data in various GitHub repos. And we wanted to put everything we already had straight into DevHub, which is this big job to get it all munged into a nice format, publish it out into the static website, build for DevHub itself. And this is something where Claude Code really shines because it's mapping, it's data mapping.

Like it's what 99% of programming is. There's heaps of examples out of everywhere. You have sample data, you have a destination schema. So I had great success just using it to color in the lines for this. So for instance, for this one here, this is the schema for content items. That end up in DevHub and I got this.

Basically by just giving [00:24:00] it the schema from Astro, so the website folks from Datadog built and Astro ui and the content catalog with static content in it with all the different fields they need for one side. And on the other side we have all of these data sources I mentioned, like the different agent integration data sets we have and library integration data sets.

And I basically just put it all in files and gave it to Claude and said Hey, can you go and write me type definitions for this for TypeScript? And I did it and it was perfect, and it saved me a whole pile of time. Nice. 

[00:24:29] **Jason Hand:** Yeah. I love Astro as you mentioned, like our team's been kinda migrating some stuff over to Astro.

We just migrated our site. The AI tools Labs is now an Astro site. I just migrated my personal site to an Astros site as well. So yeah, it's a, it's the new hotness. It's fun. It's nice that the models do understand Astro pretty well, which is cool. 

[00:24:50] **Scott Gerring:** Yeah, and it works really well if you have a TypeScript thing that's producing the data and then you can TypeScript it up on Astro as well. My personal blog is also built with Astro and I also migrated it about a year [00:25:00] ago, and I. 

[00:25:02] **Jason Hand:** Seems like we've got a little Astro community, Starting to, you come together here. 

[00:25:06] **Scott Gerring:** It's, I think it's I don't have such strong opinions about web development, but it seems if you want something that works well and you don't have very strong opinions about web dev it is just perfect.

I love it. 

[00:25:15] **Jason Hand:** Yeah. It's fast. It can shrink down like images and make your site faster and make some of that better. And just what's the word? Where there's like components you can drop things in.

Modular, perhaps. Modular. That's what I'm looking for. Yeah. Yeah. It's, I love it. 

[00:25:28] **Scott Gerring:** I like it a lot 

[00:25:28] **Jason Hand:** as well. Go check out Astro. 

[00:25:30] **Scott Gerring:** Yeah, everyone get Astro. Yeah. And so a similar sort of thing part of the import pipeline has all these libraries that we have. So if you go and build an integration to Datadog, to pull metrics from some particular system that exposes metrics, maybe a fancy hardware router device for instance.

We would link to a GitHub repository that contains the code for that. And as part of that, we wanna pull back a pile of metadata from GitHub, when it was it last updated, how many stars does it have, and so on. And for this, I needed a GitHub client. So I [00:26:00] gave Claude code, the API document link and said, go and write me a GitHub client, please.

And again it just did it. It was great. I looked at the output. I couldn't fault it. It was, if anything better documented than what I would do myself. And I think things like this that really shines, like it's such a mechanical task. It's not something that needs. Heaps of brain capacity.

It's not something where there's heaps of variation within what you get out of it. It's just go forth and do it. And if you're limited by the speed of your typing, it takes a bit longer than if you're not. Yep. So I was really happy with that, and I find that it's also much, much better and more consistent with things like JavaScript and TypeScript with Rust, but I suspect that's just because there's a lot more of that in the training set.

It also works really well for refactoring TypeScript. I found that's something that I at least don't have a good IDE for, like I don't have something, for rust or for Java or whatnot. You have the jet brains tools. You wanna move things between packages, rename them. It's a couple of mouse clicks or some keystrokes.

It all works very [00:27:00] well. I find that you can just go to Claude and say, Hey, I'm not quite happy with this package layout. I would like to change it like this one. Do you think this is sensible? And two, go forth and do it for me. And it does it, it takes longer than if you have really fancy refactoring tools, but it's makes something that would otherwise be painful.

Very easy. Yeah. 

[00:27:20] **Jason Hand:** Which I think is the perfect, like use case for a lot of these things. That's where I'm using Gene A Ai I the most is just make my life easier. Do the stuff I don't really, I know needs to be done. I know it'll make life and everything else easier moving forward, but it takes time to slog to get through it.

Yeah. 

[00:27:39] **Scott Gerring:** Yeah, and a lot of these things aren't actually fun. I think there's lots of elements of programming that are inherently very creative and it's a pity to offload that to a tool also because it helps you flesh out your model of the problem you're trying to solve if you do it yourself.

But heaps and heaps of programming is just boring, repetitive work. That these things really shine a

[00:27:58] **Jason Hand:** hundred [00:28:00] percent. 

[00:28:01] **Scott Gerring:** This one's a bit harder to explain. I dunno if the text I've got is gonna be very interesting. So as part of this dev hub thing it's just transforming things between different data formats, like I mentioned.

And for that you really want to have tests. You want some unit tests that are like, if I put this in to the machine that does the transforming, I get this out. I'm a sensible programmer, but I don't write JavaScript or TypeScript very much, and I'm not up, up to date with the latest tooling for all of that.

So it's can you go and write me a unit test so I can fill in the details myself? Delegating the scaffolding of the unit test to Claude. This was an absolute mess. I, and I think it's, like I say, I'm not a frontend person. It's it's really hard for me to judge how inherently hard the problem of hooking a unit test library up to a TypeScript no JS project is, but it went down this crazy rabbit hole.

I dunno if I captured it here, where. It kept trying to change the module [00:29:00] packaging all over the place, and because DevHub is this nested multi module thing, it was changing it at different levels. I'll change this to ES Next and I'll change this to common JS and breaking everything in like really complicated, strange ways.

Because JavaScript module seems to be a very complicated thing, but in the end, the test, it had a test that ran, but like everything else was completely broken. So I looked at it and I was like, you've changed this thing that seems like it's probably a big deal. Can you tell me about that? And it explained itself, but I can't get myself into a situation where I'm comfortable with what it's done or I fully understand the implications of what it's done.

So I just gave up on it. I said, this is outside of my skillset and you haven't convinced me that it's within your skillset. But I found it really funny because it's like you give it a small task and it makes this dramatic project wide impacting choice without really telling you about it until you notice that everything else has been broken.

But yeah I think also if you were, you notice those ones quickly because you [00:30:00] go and look at the change set and all these files you weren't expecting to be touched have been touched. And then it's, the blast radius is naturally limited that way for sure. These two are a bit weirder.

I'll explain this one quickly with my words because I don't think the text will help very much. Okay. I tried to clean it up a bit before, but I just confused myself. I just published this tool on our GitHub that does architectural unit testing for rust. So you can, there, there's similar tools for Java and DotNet you can write assertions about.

If. I have code in the rest API package, it shouldn't be using the database package directly. Or if I have code in the database package, it shouldn't have global static variables, stuff like that. Stuff that people have in their heads when they build systems, but you don't really have a way to encode them.

But the interesting fact about this from a generative AI perspective is that. To build things like this in the Rust-iverse, you have to use or you don't have to. But the easiest way to do it is to use the internals of rust itself. So like the compiler internals, [00:31:00] which you can do, but it's all very gnarly and not very documented, unsurprisingly.

It's also unstable. So I tried initially a bit when I started with Claude and with Cursor to let it work on this project. Hey, here's a lint that limits what things in this namespace can get to it in this other namespace. It just can't it tries, it runs into the wall. It can't get interfaces, right?

It doesn't understand the shape of the code or how to work with it. You just have to give up basically. I think this is reasonable as well though, because it's just, there's nothing about this anywhere. It's not something that's massively present in the training set. You can try and give it the source code of the Rust Compiler for the interfaces it's trying to use, but because these files are enormous, it'll immediately fill the whole context window and then some, and that doesn't really work either.

I think maybe you could be really clever and strip out all the new lines and the comments and that and see if it could go with that. It's the only like truly pathological case I've [00:32:00] found so far, where the tools are just not helpful at all. 

[00:32:04] **Jason Hand:** When you are running Claude Code, you know how when you load Claude Code from whichever director you're in Claude Code has like an understanding of everything below that, right? So you can always sometimes I'll realize I need to go one more level up actually and give it more context. Of like the broader project. Did you run into anything like that or did you always start in like the most top tier place to load?

Claude Code, 

[00:32:28] **Scott Gerring:** I often do it from a level down rather than a level up in the sense that there's a few projects I'm working on where they're like Monorepo microservice. So there's this surrounding context that has, infrastructure as code files deployment stuff for Readme, and then you have a bunch of sub-services.

Then I find that if I start Claude Code in like the directory of a subservice and I'm working on something that's only relevant to the subservice it gets us confused. 'cause then it's just, you've basically said like this is the thing you are considering. But then, for anything cross [00:33:00] cutting, you get your claude.md in the top and you explain, here are all my different services.

If I ask you something about this then go look there. 

Yeah, it, I think about that a lot. Like what you let it see initially really shapes its perception of the world. 

[00:33:14] **Jason Hand:** It really does. Yeah. And that and I think, whether you're trying to save token usage here and there, you make decisions on what context you end up giving it, which is gonna have a direct impact to the output.

[00:33:26] **Scott Gerring:** Yeah.

I think I am not nearly as frugal as I should be with the token usage stuff. Yeah. I, this is a bit the curse of having something like this bundled up from a nice subscription through your employer. I think it probably also has a performance issue though. Hey, if you run Claude Code, where it's always sitting on a hundred percent context window size, I assume that makes everything much slower, and that's my perception of it as well.

[00:33:51] **Jason Hand:** Yeah. I feel like there has to be a push in, in the direction where rather than everybody in an organization [00:34:00] all like bringing down and pushing up and bringing down and pushing up context all the time, that somehow all of us are just sharing the same context. Yeah. You know what I mean? Because we're just we're just wasting passing data back and forth.

It usually all sits in. In the place it always needs to be, I think, but getting it into the models, of course sending it into wherever you're sending it is the expensive part, but I feel like we, that's a problem we're gonna have to solve for because code's not getting code, bases aren't getting necessarily smaller.

[00:34:31] **Scott Gerring:** There's lots of interesting kind of intermediate representations for query and code these days. Code QL is one where, you run it over your code base and it builds a little embedded database that kind of describes the structure of it. Syntax tree style, but abstracted a bit back from the concrete language.

I wonder if things are gonna go in that direction rather than just jamming enormous strings into context everywhere. Being a bit fancier. Yeah, you're right. There's a lot of rediscovery effects about code just because the tools tend to [00:35:00] work primarily with big string buffers. Yeah. It seems like there's a lot of reasonably low hanging fruit there to push things in the right direction.

[00:35:10] **Jason Hand:** MCP is a good example of, I think us all just solving the problems that we continue to, bump into. It's oh, now we need some other way to interface with all these other tools and so not us Anthropic people much smarter than me. I'm just a, I'm just a beneficiary. But , there's gonna be more of that, right?

I just think we're gonna continue finding ways around the roadblocks that we hit. 

[00:35:30] **Scott Gerring:** Yeah, absolutely. And I think MCP also for us in the Datadog sense is really interesting because you can imagine some really cool use cases for it. For instance you, you have the situation where you have Cursor open and a unit test broken, you say.

You can see the broken unit test, go and fix it. With MCP, you could say, Hey, you can see this production issue. You have all this different observability data about it. Go and fix it. I think that's really cool. I haven't had much time to play with it yet, but like it could be [00:36:00] one of those things that feels very akin to magic when it works well.

[00:36:04] **Jason Hand:** Yeah. All right. 

[00:36:06] **Scott Gerring:** Cool. Anything else you wanna share before we wrap things up? The absolute last one I have, and this isn't very interesting in comparison to the code ones, I think, is, I have this fancy writing tablet. I dunno if you have one of these. Okay. It's a SuperNote, but it's like an ola sorry, a okay.

E ink thing you can write on. And I use it constantly now because it's I dunno. I like the tactility of it or something, but it's, yeah I write a lot for work, obviously, and I've taken to writing on this rather than typing so I can go sit outside in the hammock or whatnot and just write a little bit hammock driven development style.

Okay. And what I've taken to doing with it is exporting the thing I've written as a PDF and giving it to either Claude code or chat GPT. As well as the context of say, my existing blog writing and saying, Hey, can you go and transcribe all of this, fix all the spelling mistakes Australian English spelling.

Thank you very much. And if anything is weird or [00:37:00] stylistically unusual about it, highlight it for me. And that works really well. It saves so much time and it fixes, like in, in the old days where you would just do. Like handwriting recognition. It fixes all of the weird little issues because your handwriting's awful, or you can't spell properly or whatnot.

That's awesome. It almost feels like a boring use case in comparison to everything else, oh, 

[00:37:19] **Jason Hand:** That's a, I love that. I've got something similar. I've got I forget what it's called but a little tablet that I can write on also and export PDFs out. So yeah, you might have to gimme more details on step by step what you're doing there.

I'll try it with my stuff too 

[00:37:33] **Scott Gerring:** Yeah, absolutely. But it's so simple and unexciting, but it's just, it just works. It makes my life, I think anything that solves 

[00:37:38] **Jason Hand:** little problems for me is some of the most exciting stuff. 

[00:37:41] **Scott Gerring:** Yeah. Fair enough.

[00:37:45] **Jason Hand:** Cool. Scott, this has been awesome. Thanks for giving us a little bit of an update on additional findings that you've come across as you experiment with Claude Code.

I think a lot of what you have is [00:38:00] so unique to Rust which is nice. I don't get a lot of exposure to Rust, so it's good to hear some just what you're seeing and learning as you start to experiment. So I can add that to my own knowledge. I think 

[00:38:12] **Scott Gerring:** it's also, it works comparatively very well for things like TypeScript.

So the weird things that I like to talk about tend to be more in rust. Yeah. Cool. Yeah. Anyway, it's been a pleasure as always. Yeah. 

[00:38:23] **Jason Hand:** Thanks for thanks for getting on and talking to me and yeah, hopefully we'll do it again sometime in the near future. 

[00:38:29] **Scott Gerring:** Absolutely. A lovely day."
---

In this candid engineering chat, Datadog developer advocate Scott Gerring walks Jason Hand through his latest experiments with Anthropic's Claude Code assistant. Gerring shows off a ray-traced teapot scene that Claude generated almost entirely on its own. By conversationally prescribing high-level architecture and letting the model write the Rust, he ended up with clean, compiler-checked interfaces for materials, impressive Lambertian shading and glass refractions, and a working multithreaded renderer—all things that would have sounded like science-fiction two years ago. Yet the demo also exposes Claude's more comedic failure modes: a 'random' axis picker implemented as a global static counter wrapped in an unsafe block, a crippling global mutex that turns a progress bar into a five-times performance penalty, and subtle rendering artifacts caused by omitting the classic ray-origin epsilon offset. Beyond graphics, Gerring describes using Claude Code as a pragmatic coding mule. It rapidly scaffolded a TypeScript ETL pipeline that ingests Datadog integration metadata into the new DevHub site powered by Astro, complete with strongly-typed schemas and a bespoke GitHub API client. The assistant excelled at mechanical mapping and refactoring chores that normally burn hours of human focus, but it stumbled when asked to wire up Jest unit-tests—introducing brittle module-system changes that broke the entire build. At the bleeding edge, Claude still falters on Rust compiler internals for architectural linting, reminding us that engineers must recognise success criteria and domain pitfalls before blindly shipping AI-generated code. The pair close by musing about shared organisational context windows, multi-modal workflows like handwriting drafts on a Supernote then piping PDFs through Claude for flawless transcription, and the exciting potential of Datadog's new MCP for autonomous incident remediation. The through-line: lean on generative AI for the repetitive drudgery, but keep seasoned intuition—plus good observability—at the helm.